{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EGen-Core — Model Evaluation: ELO Tournament\n",
                "\n",
                "**The Athena Project (2025–2026)** — Developed by [ErebusTN](https://github.com/ErebusTN)\n",
                "\n",
                "This notebook implements an ELO rating tournament for comparing language models\n",
                "loaded through EGen-Core. It uses translated Vicuna evaluation prompts to\n",
                "benchmark models head-to-head.\n",
                "\n",
                "## Overview\n",
                "\n",
                "1. **Define evaluation prompts** (Vicuna-style)\n",
                "2. **Load models** via `AutoModel.from_pretrained()`\n",
                "3. **Run pairwise comparisons** — each model generates a response for the same prompt\n",
                "4. **Compute ELO ratings** using a round-robin tournament\n",
                "5. **Display results** in a ranked leaderboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q EGen-Core pandas tabulate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import pandas as pd\n",
                "import random\n",
                "import math\n",
                "import sys\n",
                "\n",
                "print(f\"Python:  {sys.version}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA:    {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Evaluation Prompts\n",
                "\n",
                "Vicuna-style evaluation prompts covering reasoning, knowledge, coding, and creativity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EVAL_PROMPTS = [\n",
                "    # Knowledge\n",
                "    \"What are the main differences between nuclear fission and nuclear fusion?\",\n",
                "    \"Explain the concept of supply and demand in economics.\",\n",
                "    \"What is the greenhouse effect and how does it relate to climate change?\",\n",
                "    \n",
                "    # Reasoning\n",
                "    \"If a train travels 120 km in 2 hours, what is its average speed? Show your reasoning.\",\n",
                "    \"A farmer has 17 sheep. All but 9 die. How many sheep are left? Explain step by step.\",\n",
                "    \n",
                "    # Coding\n",
                "    \"Write a Python function to check if a string is a palindrome.\",\n",
                "    \"Explain the difference between a stack and a queue with examples.\",\n",
                "    \n",
                "    # Creativity\n",
                "    \"Write a short poem about artificial intelligence.\",\n",
                "    \"Describe a futuristic city in 2100 in three sentences.\",\n",
                "    \n",
                "    # Instruction Following\n",
                "    \"List 5 tips for effective time management, numbered 1-5.\",\n",
                "]\n",
                "\n",
                "print(f\"Loaded {len(EVAL_PROMPTS)} evaluation prompts.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ELO Rating System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ELORatingSystem:\n",
                "    \"\"\"ELO rating system for model comparison.\"\"\"\n",
                "    \n",
                "    def __init__(self, k_factor=32, initial_rating=1500):\n",
                "        self.k_factor = k_factor\n",
                "        self.initial_rating = initial_rating\n",
                "        self.ratings = {}\n",
                "        self.match_history = []\n",
                "    \n",
                "    def add_model(self, model_name):\n",
                "        \"\"\"Register a model with initial ELO rating.\"\"\"\n",
                "        if model_name not in self.ratings:\n",
                "            self.ratings[model_name] = self.initial_rating\n",
                "    \n",
                "    def expected_score(self, rating_a, rating_b):\n",
                "        \"\"\"Calculate expected score for player A.\"\"\"\n",
                "        return 1.0 / (1.0 + math.pow(10, (rating_b - rating_a) / 400.0))\n",
                "    \n",
                "    def update_ratings(self, model_a, model_b, score_a):\n",
                "        \"\"\"Update ELO ratings after a match. score_a: 1=A wins, 0=B wins, 0.5=draw.\"\"\"\n",
                "        ra = self.ratings[model_a]\n",
                "        rb = self.ratings[model_b]\n",
                "        \n",
                "        ea = self.expected_score(ra, rb)\n",
                "        eb = self.expected_score(rb, ra)\n",
                "        \n",
                "        self.ratings[model_a] = ra + self.k_factor * (score_a - ea)\n",
                "        self.ratings[model_b] = rb + self.k_factor * ((1 - score_a) - eb)\n",
                "        \n",
                "        self.match_history.append({\n",
                "            'model_a': model_a, 'model_b': model_b,\n",
                "            'score_a': score_a, 'new_rating_a': self.ratings[model_a],\n",
                "            'new_rating_b': self.ratings[model_b]\n",
                "        })\n",
                "    \n",
                "    def get_leaderboard(self):\n",
                "        \"\"\"Return sorted leaderboard as a DataFrame.\"\"\"\n",
                "        data = sorted(self.ratings.items(), key=lambda x: x[1], reverse=True)\n",
                "        return pd.DataFrame(data, columns=['Model', 'ELO Rating'])\n",
                "\n",
                "# Test the ELO system\n",
                "elo = ELORatingSystem()\n",
                "elo.add_model('Model_A')\n",
                "elo.add_model('Model_B')\n",
                "elo.update_ratings('Model_A', 'Model_B', 1.0)  # A wins\n",
                "print(f\"After A beats B: {elo.ratings}\")\n",
                "print(\"PASS: ELO rating system functional.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Loading and Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from egen_core import AutoModel\n",
                "\n",
                "def load_model(model_id, **kwargs):\n",
                "    \"\"\"Load a model through EGen-Core's AutoModel.\"\"\"\n",
                "    print(f\"Loading {model_id}...\")\n",
                "    model = AutoModel.from_pretrained(model_id, **kwargs)\n",
                "    print(f\"  Loaded successfully.\")\n",
                "    return model\n",
                "\n",
                "def generate_response(model, prompt, max_new_tokens=100, max_length=256):\n",
                "    \"\"\"Generate a response from a loaded EGen-Core model.\"\"\"\n",
                "    tokens = model.tokenizer(\n",
                "        [prompt],\n",
                "        return_tensors=\"pt\",\n",
                "        return_attention_mask=False,\n",
                "        truncation=True,\n",
                "        max_length=max_length,\n",
                "        padding=False\n",
                "    )\n",
                "    \n",
                "    output = model.generate(\n",
                "        tokens['input_ids'].cuda(),\n",
                "        max_new_tokens=max_new_tokens,\n",
                "        use_cache=True,\n",
                "        return_dict_in_generate=True\n",
                "    )\n",
                "    \n",
                "    return model.tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"Model loading and generation functions defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Tournament Runner\n",
                "\n",
                "Configure the models to evaluate below. Each model will be compared against every\n",
                "other model on each prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure models for tournament\n",
                "# Set to actual HuggingFace model repo IDs to run a real tournament\n",
                "MODEL_IDS = [\n",
                "    # \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
                "    # \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
                "]\n",
                "\n",
                "def score_response(response_a, response_b):\n",
                "    \"\"\"Simple length-based scoring heuristic for demo purposes.\n",
                "    In production, use GPT-4 or human evaluation.\"\"\"\n",
                "    len_a = len(response_a.strip())\n",
                "    len_b = len(response_b.strip())\n",
                "    if len_a > len_b * 1.2:\n",
                "        return 1.0  # A wins\n",
                "    elif len_b > len_a * 1.2:\n",
                "        return 0.0  # B wins\n",
                "    else:\n",
                "        return 0.5  # Draw\n",
                "\n",
                "if len(MODEL_IDS) >= 2 and torch.cuda.is_available():\n",
                "    elo = ELORatingSystem()\n",
                "    models = {}\n",
                "    \n",
                "    for model_id in MODEL_IDS:\n",
                "        elo.add_model(model_id)\n",
                "        models[model_id] = load_model(model_id)\n",
                "    \n",
                "    # Round-robin tournament\n",
                "    for prompt_idx, prompt in enumerate(EVAL_PROMPTS):\n",
                "        print(f\"\\n--- Prompt {prompt_idx + 1}/{len(EVAL_PROMPTS)} ---\")\n",
                "        print(f\"  {prompt[:80]}...\")\n",
                "        \n",
                "        for i in range(len(MODEL_IDS)):\n",
                "            for j in range(i + 1, len(MODEL_IDS)):\n",
                "                resp_a = generate_response(models[MODEL_IDS[i]], prompt)\n",
                "                resp_b = generate_response(models[MODEL_IDS[j]], prompt)\n",
                "                score = score_response(resp_a, resp_b)\n",
                "                elo.update_ratings(MODEL_IDS[i], MODEL_IDS[j], score)\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"FINAL LEADERBOARD\")\n",
                "    print(\"=\"*60)\n",
                "    print(elo.get_leaderboard().to_string(index=False))\n",
                "else:\n",
                "    print(\"SKIP: Add at least 2 model IDs to MODEL_IDS and ensure CUDA is available.\")\n",
                "    print(\"\\nDemo leaderboard (simulated):\")\n",
                "    elo = ELORatingSystem()\n",
                "    demo_models = ['Model_Alpha', 'Model_Beta', 'Model_Gamma']\n",
                "    for m in demo_models:\n",
                "        elo.add_model(m)\n",
                "    # Simulate matches\n",
                "    elo.update_ratings('Model_Alpha', 'Model_Beta', 1.0)\n",
                "    elo.update_ratings('Model_Alpha', 'Model_Gamma', 1.0)\n",
                "    elo.update_ratings('Model_Beta', 'Model_Gamma', 0.5)\n",
                "    print(elo.get_leaderboard().to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**ELO Tournament framework ready.** ✅ Configure `MODEL_IDS` with real HuggingFace\n",
                "model repos to run a full evaluation. For production use, replace `score_response()` with\n",
                "GPT-4 or human evaluation grading."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}