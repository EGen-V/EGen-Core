
from transformers import GenerationConfig

from .tokenization_baichuan import BaichuanTokenizer

from .egen_core_base import EGenCoreBaseModel



class EGenCoreBaichuan(EGenCoreBaseModel):


    def __init__(self, *args, **kwargs):


        super(EGenCoreBaichuan, self).__init__(*args, **kwargs)

    def get_use_better_transformer(self):
        return False
    def get_tokenizer(self, hf_token=None):
        # use this hack util the bug is fixed: https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/discussions/2
        return BaichuanTokenizer.from_pretrained(self.model_local_path, use_fast=False, trust_remote_code=True)

    def get_generation_config(self):
        return GenerationConfig()


